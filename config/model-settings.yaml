# vLLM Model Configuration for Bantz
# Epic LLM-1 (Issue #155): vLLM Dual-Server Infrastructure
#
# Strategy: SEQUENTIAL (due to RTX 4060 8GB VRAM constraints)
# - 3B model (port 8001): Fast router/orchestrator, always-on baseline
# - 7B model (port 8002): High-quality chat (manual switch, requires stopping 3B)
#
# RTX 4060 8GB VRAM Constraints:
#   - 3B AWQ: ~3.0GB VRAM (concurrent safe)
#   - 7B AWQ: ~5.6GB VRAM (exclusive mode, requires 3B stopped)
#
# Startup Scripts:
#   ./scripts/start_vllm_3b.sh   # Start 3B router (baseline)
#   ./scripts/start_vllm_7b.sh   # Start 7B chat (stops 3B first)
#   ./scripts/stop_vllm_servers.sh

model_strategy: sequential  # Sequential mode: 3B or 7B, not both simultaneously

models:
  router:
    name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
    provider: "vllm"
    context_window: 2048
    quantization: null  # FP16 (fits in VRAM)
    endpoint: "http://localhost:8001/v1"
    port: 8001
    quantization: "awq_marlin"
    max_model_len: 2048
    gpu_memory_utilization: 0.70
    vram_usage_gb: 3.0
    ttft_ms: 41  # Validated (vllm_validation_report.md)
    role: "routing, planning, tool selection"
    enabled: true
    
  orchestrator:
    name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
    provider: "vllm"
    endpoint: "http://localhost:8001/v1"  # Same as router (single 3B server)
    port: 8001
    context_window: 2048
    quantization: "awq_marlin"
    
  chat:
    name: "Qwen/Qwen2.5-7B-Instruct-AWQ"
    provider: "vllm"
    endpoint: "http://localhost:8002/v1"
    port: 8002
    context_window: 2048
    quantization: "awq_marlin"
    max_model_len: 2048
    gpu_memory_utilization: 0.65
    vram_usage_gb: 5.6
    ttft_ms: 65  # Estimated (not yet benchmarked)
    role: "chat, reasoning, complex responses"
    enabled: false  # Sequential mode - manual switch required

# Single-model fallback (for comparison or resource-constrained setups)
single_model:
  name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
  provider: "vllm"
  endpoint: "http://localhost:8001/v1"
  context_window: 2048
  quantization: "awq_marlin"

# =============================================================================
# Router Configuration
# =============================================================================
router:
  temperature: 0.0  # Deterministic routing
  max_tokens: 128   # Sufficient for JSON output
  top_p: 1.0
  stop_sequences:
    - "}"
    - "\n\n"
  
  # Prompt budget
  max_system_prompt_tokens: 500
  max_user_input_tokens: 100
  total_budget: 728  # system + user + output
  
  # Performance targets
  targets:
    p50_latency_ms: 100
    p95_latency_ms: 150
    json_validity_percent: 99
    throughput_tokens_per_sec: 100

# =============================================================================
# Orchestrator Configuration  
# =============================================================================
orchestrator:
  temperature: 0.0  # Deterministic planning
  max_tokens: 256   # Extended JSON with reasoning
  top_p: 1.0
  stop_sequences:
    - "}"
  
  # Prompt budget
  max_system_prompt_tokens: 700
  max_user_input_tokens: 100
  max_dialog_summary_tokens: 200
  total_budget: 1256
  
  # Performance targets
  targets:
    p50_latency_ms: 120
    p95_latency_ms: 200
    json_validity_percent: 98
    throughput_tokens_per_sec: 80

# =============================================================================
# Chat Configuration
# =============================================================================
chat:
  temperature: 0.2  # Slight creativity for natural responses
  max_tokens: 200   # Short, concise responses (Jarvis style)
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  
  # Prompt budget
  max_system_prompt_tokens: 400
  max_conversation_history_tokens: 500
  max_user_input_tokens: 100
  total_budget: 1400
  
  # Performance targets
  targets:
    p50_latency_ms: 200
    p95_latency_ms: 400
    throughput_tokens_per_sec: 50

# =============================================================================
# Memory / Context Management
# =============================================================================
memory:
  # Rolling dialog summary
  max_dialog_summary_tokens: 500
  summary_style: "compact"  # 1-2 sentences per turn
  
  # Tool results
  max_tool_results_tokens: 300
  summarize_threshold: 200  # Summarize if result > 200 tokens
  
  # Session context
  max_session_context_tokens: 200
  
  # PII filtering
  pii_filter_enabled: true
  pii_patterns:
    - email
    - phone
    - credit_card
    - ssn

# =============================================================================
# vLLM Server Configuration (Updated for Split Strategy - Issue #153)
# =============================================================================
vllm_server:
  # Option 1: Single vLLM instance with model switching (Recommended)
  base_url: "http://127.0.0.1:8001"
  api_key: "EMPTY"
  timeout_seconds: 30
  max_retries: 2
  
  # Server startup for split strategy (load 8B, use 3B via separate endpoint if needed)
  # python -m vllm.entrypoints.openai.api_server \
  #   --model Qwen/Qwen2.5-8B-Instruct \
  #   --port 8001 \
  #   --dtype auto \
  #   --max-model-len 4096 \
  #   --gpu-memory-utilization 0.85 \
  #   --max-num-seqs 16
  
  # Option 2: Separate endpoints for 3B and 8B (if parallel needed)
  # endpoints:
  #   small: "http://127.0.0.1:8001"  # 3B for router/orchestrator
  #   large: "http://127.0.0.1:8002"  # 8B for chat
  
  # RTX 4060 Optimization
  gpu_memory_utilization: 0.85  # Leave headroom for KV cache (5.5GB peak + 2.5GB buffer)
  max_model_len: 4096           # Sufficient for most conversations
  max_num_seqs: 16              # Batch size for throughput
  enable_prefix_caching: true   # Speed up repeated prompts
  
  # Quantization (if VRAM tight)
  # quantization: "awq"  # Saves ~40% VRAM with 10-15% latency increase

# =============================================================================
# RTX 4060 Hardware Monitoring (Issue #153)
# =============================================================================
hardware:
  gpu_model: "NVIDIA RTX 4060"
  vram_total_mb: 8192
  vram_target_usage_mb: 7000  # Leave 1GB for system/buffers
  
  monitoring:
    enabled: true
    check_interval_seconds: 10
    alert_threshold_mb: 7500  # Alert if VRAM > 7.5GB
    
  ttft_targets:  # Time-To-First-Token (Jarvis feeling!)
    router_p95_ms: 200
    orchestrator_p95_ms: 200
    chat_p95_ms: 300

# =============================================================================
# Benchmarking Configuration
# =============================================================================
benchmark:
  default_iterations: 30
  scenarios:
    router:
      - "hey bantz nasılsın"
      - "bugün neler yapacağız"
      - "saat 4 için toplantı oluştur"
      - "bu akşam neler yapacağız"
      - "kendini tanıt"
    
    orchestrator:
      - name: "smalltalk"
        input: "hey bantz nasılsın"
      - name: "calendar_list_today"
        input: "bugün neler yapacağız bakalım"
      - name: "calendar_create"
        input: "saat 4 için bir toplantı oluştur"
    
    chat:
      - "merhaba"
      - "nasılsın"
      - "kim yarattı seni"
  
  # Metrics to track
  metrics:
    - latency_p50
    - latency_p95
    - latency_p99
    - throughput_tokens_per_sec
    - json_validity_percent
    - success_rate_percent

# =============================================================================
# Two-Model Strategy (Future Upgrade)
# =============================================================================
# Uncomment this section when upgrading to two-model approach
#
# router_model:
#   name: "Qwen/Qwen2.5-1.5B-Instruct"  # Smaller, faster for routing
#   temperature: 0.0
#   max_tokens: 64
#   total_budget: 364
#
# orchestrator_chat_model:
#   name: "Qwen/Qwen2.5-3B-Instruct"  # Keep current model for complex tasks
#   # ... (use orchestrator/chat settings above)

# =============================================================================
# Notes
# =============================================================================
# 
# Single-Model Rationale:
# - Consistency: Same "personality" across all interactions
# - Simplicity: 1 model = 1 GPU = easier deployment
# - Speed: 3B model fast enough for Jarvis UX (<200ms p95)
# - Cost: Lower than 7B, acceptable for MVP
#
# Future Considerations:
# - If router latency > 150ms: Switch to 1.5B for router only
# - If chat needs better quality: Upgrade to 7B for orchestrator/chat
# - If multi-user: Implement batch inference or multiple instances
