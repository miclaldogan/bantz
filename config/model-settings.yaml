# vLLM Model Configuration for Bantz
#
# Current strategy:
# - Local: vLLM 3B on port 8001 for fast routing/planning/tool selection
# - Quality writing: Gemini (cloud) when enabled via env (see docs/setup/vllm.md)
#
# Note: Local 7B/8B is intentionally NOT part of the default workflow
# due to typical laptop VRAM constraints.
#
# Startup Scripts:
#   ./scripts/vllm/start_3b.sh   # Start 3B router (baseline)
#   ./scripts/vllm/stop.sh

model_strategy: single  # Single local model (3B). Quality tier handled via Gemini.

models:
  router:
    name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
    provider: "vllm"
    context_window: 4096
    endpoint: "http://localhost:8001/v1"
    port: 8001
    quantization: "awq_marlin"
    max_model_len: 4096
    gpu_memory_utilization: 0.70
    vram_usage_gb: 3.0
    ttft_ms: 41  # Validated (vllm_validation_report.md)
    role: "routing, planning, tool selection"
    enabled: true
    
  orchestrator:
    name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
    provider: "vllm"
    endpoint: "http://localhost:8001/v1"  # Same as router (single 3B server)
    port: 8001
    context_window: 4096
    quantization: "awq_marlin"
    
  chat:
    name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
    provider: "vllm"
    endpoint: "http://localhost:8001/v1"
    port: 8001
    context_window: 4096
    quantization: "awq_marlin"
    max_model_len: 4096
    gpu_memory_utilization: 0.70
    vram_usage_gb: 3.0
    ttft_ms: 41  # Same server as router
    role: "chat (local baseline); upgrade via Gemini quality tier when enabled"
    enabled: true

# Single-model fallback (for comparison or resource-constrained setups)
single_model:
  name: "Qwen/Qwen2.5-3B-Instruct-AWQ"
  provider: "vllm"
  endpoint: "http://localhost:8001/v1"
  context_window: 4096
  quantization: "awq_marlin"

# =============================================================================
# Router Configuration
# =============================================================================
router:
  temperature: 0.0  # Deterministic routing
  max_tokens: 256   # Issue #418: increased from 128 to avoid JSON truncation with slim schema
  top_p: 1.0
  boost_floor: 0.3  # Issue #889: min confidence to allow auto-boost (env: BANTZ_ROUTER_BOOST_FLOOR)
  stop_sequences:
    # IMPORTANT: Never use "}" alone as a stop sequence — it truncates JSON
    # at the first nested closing brace instead of the root object's.
    # The actual stop tokens are hardcoded in llm_router.py for safety.
    # These are kept here for documentation only.
    - "\nUSER:"
    - "\n\nUSER:"
    - "\n---"
  
  # Prompt budget
  max_system_prompt_tokens: 500
  max_user_input_tokens: 100
  total_budget: 728  # system + user + output
  
  # Performance targets
  targets:
    p50_latency_ms: 100
    p95_latency_ms: 150
    json_validity_percent: 99
    throughput_tokens_per_sec: 100

# =============================================================================
# Orchestrator Configuration  
# =============================================================================
orchestrator:
  temperature: 0.0  # Deterministic planning
  max_tokens: 256   # Extended JSON with reasoning
  top_p: 1.0
  stop_sequences:
    - "}"
  
  # Prompt budget
  max_system_prompt_tokens: 700
  max_user_input_tokens: 100
  max_dialog_summary_tokens: 200
  total_budget: 1256
  
  # Performance targets
  targets:
    p50_latency_ms: 120
    p95_latency_ms: 200
    json_validity_percent: 98
    throughput_tokens_per_sec: 80

# =============================================================================
# Chat Configuration
# =============================================================================
chat:
  temperature: 0.2  # Slight creativity for natural responses
  max_tokens: 200   # Short, concise responses (Jarvis style)
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  
  # Prompt budget
  max_system_prompt_tokens: 400
  max_conversation_history_tokens: 500
  max_user_input_tokens: 100
  total_budget: 1400
  
  # Performance targets
  targets:
    p50_latency_ms: 200
    p95_latency_ms: 400
    throughput_tokens_per_sec: 50

# =============================================================================
# Memory / Context Management
# =============================================================================
memory:
  # Rolling dialog summary
  max_dialog_summary_tokens: 500
  summary_style: "compact"  # 1-2 sentences per turn
  
  # Tool results
  max_tool_results_tokens: 300
  summarize_threshold: 200  # Summarize if result > 200 tokens
  
  # Session context
  max_session_context_tokens: 200
  
  # PII filtering
  pii_filter_enabled: true
  pii_patterns:
    - email
    - phone
    - credit_card
    - ssn

# =============================================================================
# vLLM Server Configuration
# =============================================================================
vllm_server:
  # Single vLLM instance
  base_url: "http://127.0.0.1:8001"
  api_key: "EMPTY"
  timeout_seconds: 30
  max_retries: 2

  # GPU optimization
  gpu_memory_utilization: 0.85  # Leave headroom for KV cache
  max_model_len: 4096           # Sufficient for most conversations
  max_num_seqs: 16              # Batch size for throughput
  enable_prefix_caching: true   # Speed up repeated prompts
  
  # Quantization (if VRAM tight)
  # quantization: "awq"  # Saves ~40% VRAM with 10-15% latency increase

# =============================================================================
# RTX 4060 Hardware Monitoring (Issue #153)
# =============================================================================
hardware:
  gpu_model: "NVIDIA RTX 4060"
  vram_total_mb: 8192
  vram_target_usage_mb: 7000  # Leave 1GB for system/buffers
  
  monitoring:
    enabled: true
    check_interval_seconds: 10
    alert_threshold_mb: 7500  # Alert if VRAM > 7.5GB
    
  ttft_targets:  # Time-To-First-Token (Jarvis feeling!)
    router_p95_ms: 200
    orchestrator_p95_ms: 200
    chat_p95_ms: 300

# =============================================================================
# Voice Pipeline Latency Budget (Issue #427)
# =============================================================================
# Per-phase budget for ASR → Router → Tool → Finalizer → TTS pipeline.
# Used by src/bantz/core/latency_budget.py
voice_pipeline:
  latency_budget:
    asr_max_ms: 500        # Whisper ASR timeout; fallback → partial result
    router_max_ms: 100     # 3B router; fallback → pre-route cache
    tool_max_ms: 1000      # Calendar/Gmail API; fallback → async + feedback phrase
    finalizer_max_ms: 500  # Gemini quality pass; fallback → skip, use 3B
    tts_max_ms: 300        # Piper TTS; fallback → pre-cached common phrases
    end_to_end_max_ms: 2000  # Total pipeline target

  # Feedback phrases injected when a phase blocks
  feedback_phrases:
    tool: "Bir bakayım efendim..."
    finalizer: "Hemen söylüyorum..."

# =============================================================================
# Benchmarking Configuration
# =============================================================================
benchmark:
  default_iterations: 30
  scenarios:
    router:
      - "hey bantz nasılsın"
      - "bugün neler yapacağız"
      - "saat 4 için toplantı oluştur"
      - "bu akşam neler yapacağız"
      - "kendini tanıt"
    
    orchestrator:
      - name: "smalltalk"
        input: "hey bantz nasılsın"
      - name: "calendar_list_today"
        input: "bugün neler yapacağız bakalım"
      - name: "calendar_create"
        input: "saat 4 için bir toplantı oluştur"
    
    chat:
      - "merhaba"
      - "nasılsın"
      - "kim yarattı seni"
  
  # Metrics to track
  metrics:
    - latency_p50
    - latency_p95
    - latency_p99
    - throughput_tokens_per_sec
    - json_validity_percent
    - success_rate_percent

# =============================================================================
# Two-Model Strategy (Future Upgrade)
# =============================================================================
# Uncomment this section when upgrading to two-model approach
#
# router_model:
#   name: "Qwen/Qwen2.5-1.5B-Instruct"  # Smaller, faster for routing
#   temperature: 0.0
#   max_tokens: 64
#   total_budget: 364
#
# orchestrator_chat_model:
#   name: "Qwen/Qwen2.5-3B-Instruct"  # Keep current model for complex tasks
#   # ... (use orchestrator/chat settings above)

# =============================================================================
# Notes
# =============================================================================
# 
# Single-Model Rationale:
# - Consistency: Same "personality" across all interactions
# - Simplicity: 1 model = 1 GPU = easier deployment
# - Speed: 3B model fast enough for Jarvis UX (<200ms p95)
# - Cost: Lowest local footprint for MVP
#
# Future Considerations:
# - If router latency > 150ms: Switch to 1.5B for router only
# - If chat needs better quality: Enable Gemini quality tier (cloud)
# - If multi-user: Implement batch inference or multiple instances
