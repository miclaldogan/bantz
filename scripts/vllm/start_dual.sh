#!/usr/bin/env bash
# Startup script for both vLLM servers with NVIDIA driver fix
# Usage: ./scripts/vllm/start_dual.sh

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$(dirname "$SCRIPT_DIR")")"

cd "$PROJECT_ROOT"

echo "ğŸ”§ vLLM Dual Server Startup"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Check GPU
echo ""
echo "ğŸ“Š GPU KontrolÃ¼:"
if ! command -v nvidia-smi &> /dev/null; then
    echo "âŒ nvidia-smi bulunamadÄ±. NVIDIA driver yÃ¼klÃ¼ mÃ¼?"
    exit 1
fi

# Try to get GPU info (may fail due to driver mismatch)
GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>&1 || echo "Driver issue")
if [[ "$GPU_INFO" == *"Driver"* ]] || [[ "$GPU_INFO" == *"mismatch"* ]]; then
    echo "âš ï¸  NVIDIA Driver/Library version mismatch detected"
    echo "   Sunucular Ã§alÄ±ÅŸabilir ama sistem yeniden baÅŸlatma Ã¶nerilir"
    echo "   Komut: sudo reboot"
else
    echo "âœ… $GPU_INFO"
fi

# Kill existing servers
echo ""
echo "ğŸ›‘ Mevcut vLLM sunucularÄ± kapatÄ±lÄ±yor..."
pkill -f "vllm" || true
sleep 3

# Check if virtual environment exists
if [ ! -d ".venv" ]; then
    echo "âŒ Virtual environment bulunamadÄ±. Ã–nce kurun:"
    echo "   python -m venv .venv"
    echo "   source .venv/bin/activate"
    echo "   pip install -e ."
    exit 1
fi

# Activate virtual environment
source .venv/bin/activate

# Verify vLLM installation
if ! python -c "import vllm" 2>/dev/null; then
    echo "âŒ vLLM import edilemedi. Yeniden kuruluyor..."
    pip install --upgrade pip
    pip uninstall -y vllm vllm-flash-attn || true
    pip install vllm==0.6.6 --no-cache-dir
fi

echo ""
echo "ğŸš€ 3B Model BaÅŸlatÄ±lÄ±yor (Port 8001)..."
echo "   Model: Qwen/Qwen2.5-3B-Instruct-AWQ"
echo "   VRAM: ~2.5GB"

# Set LD_LIBRARY_PATH for CUDA libraries
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/iclaldogan/Desktop/Bantz/.venv/lib/python3.10/site-packages/nvidia/nvjitlink/lib

nohup python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-3B-Instruct-AWQ \
  --quantization awq_marlin \
  --dtype half \
  --port 8001 \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.40 \
  --enable-prefix-caching \
  > vllm_8001.log 2>&1 &

PID_3B=$!
echo "âœ… 3B server baÅŸlatÄ±ldÄ± (PID: $PID_3B)"

echo ""
echo "â³ 3B model yÃ¼kleniyor... (30 saniye)"
sleep 30

# Check 3B health
if curl -s http://localhost:8001/v1/models > /dev/null 2>&1; then
    echo "âœ… 3B server hazÄ±r: http://localhost:8001"
else
    echo "âš ï¸  3B server henÃ¼z hazÄ±r deÄŸil, log kontrol edin: tail -f vllm_8001.log"
fi

echo ""
echo "ğŸš€ 7B Model BaÅŸlatÄ±lÄ±yor (Port 8002)..."
echo "   Model: Qwen/Qwen2.5-7B-Instruct-AWQ"
echo "   VRAM: ~4.5GB"

nohup python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct-AWQ \
  --quantization awq_marlin \
  --dtype half \
  --port 8002 \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.50 \
  --enable-prefix-caching \
  > vllm_8002.log 2>&1 &

PID_7B=$!
echo "âœ… 7B server baÅŸlatÄ±ldÄ± (PID: $PID_7B)"

echo ""
echo "â³ 7B model yÃ¼kleniyor... (45 saniye)"
sleep 45

# Check 7B health
if curl -s http://localhost:8002/v1/models > /dev/null 2>&1; then
    echo "âœ… 7B server hazÄ±r: http://localhost:8002"
else
    echo "âš ï¸  7B server henÃ¼z hazÄ±r deÄŸil, log kontrol edin: tail -f vllm_8002.log"
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Kurulum TamamlandÄ±!"
echo ""
echo "ğŸ“ Loglar:"
echo "   3B: tail -f $PROJECT_ROOT/vllm_8001.log"
echo "   7B: tail -f $PROJECT_ROOT/vllm_8002.log"
echo ""
echo "ğŸ” Health Check:"
echo "   ./scripts/health_check_vllm.py --all"
echo ""
echo "ğŸ›‘ Durdurma:"
echo "   ./scripts/vllm/stop.sh"
echo ""
echo "âš ï¸  NOT: NVIDIA Driver/Library mismatch hatasÄ± varsa,"
echo "   sistemi yeniden baÅŸlatÄ±n: sudo reboot"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
