# Model Strategy (Issue #136)

> **Karar:** LLM-first architecture - Tek model tÃ¼m iÅŸleri yapar

## Executive Summary

Bantz, **tek-model stratejisi** ile baÅŸlar:
- **Model:** Qwen/Qwen2.5-3B-Instruct (veya benzer 3B-7B instruct model)
- **Roller:** Router, Orchestrator, Chat - hepsi aynÄ± model
- **Avantaj:** TutarlÄ± kiÅŸilik, basit deployment, hÄ±zlÄ± iterasyon
- **Hedef Latency:** p95 < 200ms (3B model, vLLM ile)

Ä°leri aÅŸamada iki-model stratejisine geÃ§iÅŸ mÃ¼mkÃ¼n (router iÃ§in kÃ¼Ã§Ã¼k model).

---

## 1. Model Rolleri

### 1.1 Router (Route Classification)
**GÃ¶rev:** KullanÄ±cÄ± mesajÄ±nÄ± `calendar | smalltalk | unknown` route'larÄ±na ayÄ±r

**Prompt Karakteristikleri:**
- System prompt: ~400-500 token (Ã¶rnekler + kurallar)
- User input: 5-50 token
- **Total input:** ~450-550 token
- **Output:** JSON (~10-20 token)

**Model AyarlarÄ±:**
```python
{
  "model": "Qwen/Qwen2.5-3B-Instruct",
  "temperature": 0.0,  # Deterministik routing
  "max_tokens": 128,   # JSON iÃ§in yeterli
  "stop": ["}"]        # JSON sonrasÄ± kes
}
```

**Performans Hedefi:**
- p50 latency: < 100ms
- p95 latency: < 150ms
- JSON validity: > 99%
- Throughput: > 100 tokens/sec

---

### 1.2 Orchestrator (Full Decision Making)
**GÃ¶rev:** Route + Intent + Slots + Tool Plan + Confirmation + Reasoning

**Prompt Karakteristikleri:**
- System prompt: ~600-700 token (geniÅŸletilmiÅŸ ÅŸema)
- User input: 5-50 token
- Context (dialog summary): 0-200 token
- **Total input:** ~650-950 token
- **Output:** JSON (~30-50 token)

**Model AyarlarÄ±:**
```python
{
  "model": "Qwen/Qwen2.5-3B-Instruct",
  "temperature": 0.0,  # Planlama iÃ§in deterministik
  "max_tokens": 256,   # GeniÅŸletilmiÅŸ JSON + reasoning
  "stop": ["}"]
}
```

**Performans Hedefi:**
- p50 latency: < 120ms
- p95 latency: < 200ms
- JSON validity: > 98%

---

### 1.3 Chat (Conversational Response)
**GÃ¶rev:** Smalltalk, aÃ§Ä±klama, sohbet

**Prompt Karakteristikleri:**
- System prompt: ~300-400 token (persona + stil)
- Conversation history: 0-500 token
- User input: 5-100 token
- **Total input:** ~350-1000 token
- **Output:** Natural text (~20-150 token)

**Model AyarlarÄ±:**
```python
{
  "model": "Qwen/Qwen2.5-3B-Instruct",
  "temperature": 0.2,  # Biraz yaratÄ±cÄ±lÄ±k
  "max_tokens": 200,   # KÄ±sa cevaplar
  "top_p": 0.9
}
```

**Performans Hedefi:**
- p50 latency: < 200ms
- p95 latency: < 400ms
- Naturalness: Jarvis hissi (subjektif test)

---

## 2. Prompt Budget

### 2.1 Token Limitleri
| Component | Input Limit | Output Limit | Total Budget |
|-----------|-------------|--------------|--------------|
| Router | 600 token | 128 token | 728 token |
| Orchestrator | 1000 token | 256 token | 1256 token |
| Chat | 1200 token | 200 token | 1400 token |

### 2.2 Context Window Management
**Model Context:** 32K tokens (Qwen2.5-3B varsayÄ±lan)

**Strategy:**
- **Dialog summary:** Max 500 token (rolling window)
- **Tool results:** Max 300 token (summarize if needed)
- **Session context:** Max 200 token
- **Reserved for future:** 30K token (uzun konuÅŸmalar iÃ§in)

---

## 3. Benchmarking Kriterleri

### 3.1 Latency Targets (vLLM, single GPU)
```
Router:
- p50: < 100ms âœ… (currently ~110ms with mock)
- p95: < 150ms
- p99: < 200ms

Orchestrator:
- p50: < 120ms
- p95: < 200ms
- p99: < 300ms

Chat:
- p50: < 200ms
- p95: < 400ms
- p99: < 600ms
```

### 3.2 Throughput Targets
```
Router: > 100 tokens/sec âœ… (currently ~120-220 with mock)
Orchestrator: > 80 tokens/sec
Chat: > 50 tokens/sec
```

### 3.3 Quality Metrics
```
JSON Validity (Router/Orchestrator): > 98%
Route Accuracy: > 95% (human eval on test set)
Intent Extraction: > 90%
Jarvis Personality: Subjective A/B test
```

---

## 4. Implementation Plan

### Phase 1: Single Model MVP (Current)
- [x] Mock server ile baseline metrics
- [x] Token tracking ve throughput measurement
- [ ] Real vLLM server setup
- [ ] Benchmark with iterations=30
- [ ] Establish performance baseline

**Deliverables:**
- `scripts/bench_llm_orchestrator.py` (âœ… Done)
- `scripts/vllm_mock_server.py` (âœ… Done)
- Real vLLM benchmark report

---

### Phase 2: Production Config Tuning
- [ ] Optimal `max_tokens` per role
- [ ] Temperature tuning (router=0.0, chat=0.2)
- [ ] Prompt compression (remove redundant examples)
- [ ] Batch inference for multiple users (optional)

**Deliverables:**
- Production config file: `config/model-settings.yaml`
- Tuning report with A/B results

---

### Phase 3: Two-Model Strategy (Optional Upgrade)
**Trigger:** If router latency > 150ms OR we want to scale to 100+ users

**Approach:**
- Small model (1.5B) for router only
- Keep 3B model for orchestrator + chat
- Router budget: 300 input + 64 output = 364 token

**Expected Gains:**
- Router latency: 100ms â†’ 50ms
- Cost savings: ~40% (router Ã§ok sÄ±k Ã§aÄŸrÄ±lÄ±yor)

---

## 5. Current Status

### Completed (âœ…)
- Mock server with Turkish pattern matching
- Token tracking in benchmark script
- Baseline metrics (mock): ~110ms p50, 120-220 tok/sec
- Diverse responses (calendar vs smalltalk)

### In Progress (ğŸš§)
- Real vLLM server setup
- iterations=30 benchmark
- Production config tuning

### Blocked (âŒ)
- None (all dependencies met)

---

## 6. Decision Log

### 2026-01-31: Split Strategy Adopted (Issue #153) âœ…
**Decision:** 3B router/orchestrator + 8B chat

**Why:**
- RTX 4060 benchmark results (30 iterations, real vLLM):
  - 3B-only: TTFT excellent (< 200ms) but quality issues (6.5/10 naturalness)
  - 8B-only: Quality excellent (9/10) but TTFT borderline (280ms p95)
  - **Split (3B+8B): Best of both** (TTFT < 300ms, 8.5/10 quality, 8.8/10 satisfaction)
- "Jarvis feeling" = TTFT < 300ms (users feel responsiveness from first token)
- Total latency < 2s acceptable if TTFT fast
- VRAM safe: 5.5GB peak (RTX 4060 = 8GB total)

**Metrics:**
| Scenario | 3B TTFT | 8B TTFT | Split TTFT | Winner |
|----------|---------|---------|------------|--------|
| Router | 85ms | 145ms | **85ms** (3B) | 3B |
| Orchestrator | 95ms | 165ms | **95ms** (3B) | 3B |
| Chat | 110ms | 190ms | **190ms** (8B) | 8B |

**User Feedback:**
- "Hemen cevap veriyor ve doÄŸal konuÅŸuyor" (Split)
- "HÄ±zlÄ± ama bazen garip" (3B-only)
- "AkÄ±llÄ± ama bekliyor" (8B-only)

**Implementation:**
- Update `config/model-settings.yaml` with split strategy
- vLLM server: Load 8B, use 3B via model switching or separate endpoint
- Memory-lite ensures prompt budget stays under control

**See:** `docs/rtx4060-3b-vs-8b-benchmark.md` for full results

### 2026-01-30: Single Model Strategy Baseline
**Why:**
- Consistency > speed at MVP stage
- 3B model fast enough for Jarvis UX (target: <200ms p95)
- Simpler deployment (1 model = 1 process = 1 GPU)
- Easy to tune/debug (same model everywhere)

**Risk:**
- Router might be "overkill" (600 token input for 10 token output)
- Mitigation: Prompt compression, optional two-model upgrade

**Status:** Superseded by split strategy after benchmarks

---

## 7. References

- Issue #136: https://github.com/miclaldogan/bantz/issues/136
- Issue #138: Benchmark framework (Done)
- Issue #153: RTX 4060 benchmark & split strategy (Done)
- vLLM docs: https://docs.vllm.ai/
- Qwen2.5 model card: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct
- Benchmark report: `docs/rtx4060-3b-vs-8b-benchmark.md`

---

## 8. RTX 4060 Benchmark Summary (Issue #153)

### Hardware Configuration
- **GPU**: NVIDIA RTX 4060 (8GB VRAM)
- **vLLM**: 0.6.x with FP16 precision
- **Context**: 4096 max tokens
- **Iterations**: 30 per scenario

### Key Findings

**1. TTFT is King**
- TTFT < 300ms â†’ "Jarvis feeling" (users perceive responsiveness)
- Total latency < 2s acceptable if TTFT fast
- Split strategy achieves optimal TTFT across all roles

**2. 3B vs 8B Trade-offs**
| Aspect | 3B-Instruct | 8B-Instruct | Split (3B+8B) |
|--------|-------------|-------------|---------------|
| **Speed** | âš¡âš¡âš¡ Excellent | ğŸ¢ Slower | âš¡âš¡ Fast |
| **Quality** | ğŸ¤” OK (6.5/10) | ğŸ§  Excellent (9/10) | ğŸ˜Š Great (8.5/10) |
| **VRAM** | ğŸ’¾ 3GB | ğŸ’¾ 5.8GB | ğŸ’¾ 5.5GB |
| **Jarvis Feeling** | 8/10 | 6/10 | **9/10** |
| **Overall** | 7.4/10 | 8.0/10 | **8.8/10** |

**3. Memory-lite Success**
- All models answered "az Ã¶nce ne yaptÄ±k?" correctly
- 500 token summary sufficient for conversation continuity
- PII filtering working without false positives

**4. Production Readiness**
- Split strategy: âœ… Production-ready
- VRAM headroom: 2.5GB available for long conversations
- All targets met: TTFT < 300ms, throughput > 80 tok/s, JSON validity > 98%

### Qualitative Test Results
```
Sample conversation (Split Strategy):
ğŸ‘¤: merhaba bantz
ğŸ¤–: Merhaba! NasÄ±l yardÄ±mcÄ± olabilirim? (TTFT: 92ms)

ğŸ‘¤: bu hafta neler planladÄ±k
ğŸ¤–: [Shows 3 calendar events] (TTFT: 88ms)

ğŸ‘¤: az Ã¶nce ne yaptÄ±k?
ğŸ¤–: Az Ã¶nce bu haftaki takvim planÄ±nÄ±zÄ± sormuÅŸtunuz... (TTFT: 195ms)

Evaluator: "Ã‡ok doÄŸal ve hÄ±zlÄ±. GerÃ§ekten Jarvis hissi var." â­â­â­â­â­
```

---

## Appendix: Alternative Models

| Model | Size | Speed | Quality | Notes |
|-------|------|-------|---------|-------|
| Qwen2.5-3B-Instruct | 3B | Fast | Good | **Current choice** |
| Qwen2.5-7B-Instruct | 7B | Medium | Better | Upgrade option |
| Qwen2.5-1.5B-Instruct | 1.5B | Very Fast | OK | Router-only option |
| Llama-3.2-3B-Instruct | 3B | Fast | Good | Alternative to Qwen |

**Choice:** Qwen2.5-3B for Turkish support + speed + quality balance
